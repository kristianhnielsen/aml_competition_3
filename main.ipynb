{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "781c1c00",
      "metadata": {},
      "source": [
        "# Columns Description\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "478d7b26",
      "metadata": {},
      "source": [
        "- HighBP - Binary - 0 = no high BP 1 = high BP\n",
        "- HighChol - Binary- 0 = no high cholesterol 1 = high cholesterol\n",
        "- CholCheck - Binary - 0 = no cholesterol check in 5 years 1 = yes cholesterol check in 5 years\n",
        "- BMI - Integer - Body Mass Index\n",
        "- Smoker - Binary - Have you smoked at least 100 cigarettes in your entire life? 0 = no 1 = yes\n",
        "- Stroke - Binary - (Ever told) you had a stroke. 0 = no 1 = yes\n",
        "- HeartDiseaseorAttack - Binary - coronary heart disease (CHD) or myocardial infarction (MI) 0 = no 1 = yes\n",
        "- PhysActivity - Binary - physical activity in past 30 days - not including job 0 = no 1 = yes\n",
        "- Fruits - Binary - Consume Fruit 1 or more times per day 0 = no 1 = yes\n",
        "- Veggies - Binary - Consume Vegetables 1 or more times per day 0 = no 1 = yes\n",
        "- HvyAlcoholConsump - Binary - Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) 0 = no 1 = yes\n",
        "- AnyHealthcare - Binary - Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc. 0 = no 1 = yes\n",
        "- NoDocbcCost -Binary - Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? 0 = no 1 = yes\n",
        "- GenHlth - Integer - Would you say that in general your health is: scale 1-5 1 = excellent 2 = very good 3 = good 4 = fair 5 = poor\n",
        "- MentHlth - Integer - Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good? scale 1-30 days\n",
        "- PhysHlth - Integer - Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? scale 1-30 days\n",
        "- DiffWalk - Binary - Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes\n",
        "- Gender - Binary - Female, Male\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2461c75",
      "metadata": {},
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "cf60b0b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, AdamW\n",
        "from keras.regularizers import L1, L2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mlflow\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df06e09",
      "metadata": {},
      "source": [
        "# Model tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "662f1dd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow.tensorflow.autolog(silent=True)  # type: ignore # That's it! 🎉\n",
        "\n",
        "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "\n",
        "# Define an experiment name\n",
        "mlflow.set_experiment(\"AML Competition 3\")\n",
        "\n",
        "import absl.logging\n",
        "\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "81198341",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column groups (similar to the notebook)\n",
        "categorical_features = [\"Gender\"]\n",
        "binary_features = [\n",
        "    \"HighBP\",\n",
        "    \"HighChol\",\n",
        "    \"CholCheck\",\n",
        "    \"Smoker\",\n",
        "    \"Stroke\",\n",
        "    \"HeartDiseaseorAttack\",\n",
        "    \"PhysActivity\",\n",
        "    \"Fruits\",\n",
        "    \"Veggies\",\n",
        "    \"HvyAlcoholConsump\",\n",
        "    \"AnyHealthcare\",\n",
        "    \"NoDocbcCost\",\n",
        "    \"DiffWalk\",\n",
        "]\n",
        "continuous_for_binning = [\"BMI\", \"Age\"]\n",
        "ordinal_features_map = {\n",
        "    \"GenHlth\": {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4}\n",
        "}  # Example mapping, adjust as needed\n",
        "numerical_features_to_scale = [\n",
        "    \"MentHlth\",\n",
        "    \"PhysHlth\",\n",
        "    \"Education\",\n",
        "    \"Income\",\n",
        "]  # Original numerical features + custom ones after creation\n",
        "\n",
        "\n",
        "# Custom domain-specific feature creation function (adapted from notebook)\n",
        "def create_domain_features_pandas(X):\n",
        "    \"\"\"Create domain-specific health features using pandas.\"\"\"\n",
        "    new_features = pd.DataFrame(index=X.index)\n",
        "\n",
        "    # Health risk score (sum of major risk factors)\n",
        "    risk_factors = [\"HighBP\", \"HighChol\", \"Smoker\", \"Stroke\", \"HeartDiseaseorAttack\"]\n",
        "    # Ensure columns exist before summing, handle potential missing columns gracefully\n",
        "    valid_risk_factors = [col for col in risk_factors if col in X.columns]\n",
        "    if valid_risk_factors:\n",
        "        new_features[\"health_risk_score\"] = X[valid_risk_factors].sum(axis=1)\n",
        "    else:\n",
        "        new_features[\"health_risk_score\"] = 0\n",
        "\n",
        "    # Lifestyle score (healthy behaviors - unhealthy behaviors)\n",
        "    healthy_behaviors = [\"PhysActivity\", \"Fruits\", \"Veggies\"]\n",
        "    unhealthy_behaviors = [\"Smoker\", \"HvyAlcoholConsump\"]\n",
        "    valid_healthy = [col for col in healthy_behaviors if col in X.columns]\n",
        "    valid_unhealthy = [col for col in unhealthy_behaviors if col in X.columns]\n",
        "\n",
        "    score = 0\n",
        "    if valid_healthy:\n",
        "        score += X[valid_healthy].sum(axis=1)\n",
        "    if valid_unhealthy:\n",
        "        score -= X[valid_unhealthy].sum(axis=1)\n",
        "    new_features[\"lifestyle_score\"] = score\n",
        "\n",
        "    # Health days interaction\n",
        "    if \"MentHlth\" in X.columns and \"PhysHlth\" in X.columns:\n",
        "        new_features[\"total_health_days\"] = X[\"MentHlth\"] + X[\"PhysHlth\"]\n",
        "        new_features[\"has_health_issues\"] = (\n",
        "            new_features[\"total_health_days\"] > 0\n",
        "        ).astype(int)\n",
        "    else:\n",
        "        new_features[\"total_health_days\"] = 0\n",
        "        new_features[\"has_health_issues\"] = 0\n",
        "\n",
        "    return new_features\n",
        "\n",
        "\n",
        "# Main preprocessing function without sklearn\n",
        "def preprocess_data_pandas(df_raw):\n",
        "    \"\"\"\n",
        "    Preprocesses the input DataFrame using pandas and numpy,\n",
        "    mimicking the sklearn pipeline steps.\n",
        "    \"\"\"\n",
        "    df = df_raw.copy()\n",
        "    processed_parts = []\n",
        "\n",
        "    # 1. One-Hot Encode Categorical Features\n",
        "    if categorical_features:\n",
        "        # Check if features exist in the dataframe\n",
        "        valid_categorical = [col for col in categorical_features if col in df.columns]\n",
        "        if valid_categorical:\n",
        "            one_hot_encoded = pd.get_dummies(\n",
        "                df[valid_categorical],\n",
        "                columns=valid_categorical,\n",
        "                prefix=valid_categorical,\n",
        "                drop_first=False,\n",
        "                dummy_na=False,\n",
        "            )  # Keep all categories, don't drop first to match sparse=False, handle_unknown='ignore' implicitly\n",
        "            processed_parts.append(one_hot_encoded)\n",
        "            # Drop original categorical columns\n",
        "            df = df.drop(columns=valid_categorical)\n",
        "\n",
        "    # 2. Bin Continuous Features (Quantile Binning + One-Hot)\n",
        "    n_bins = 5\n",
        "    for col in continuous_for_binning:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                # Quantile binning\n",
        "                binned_col, _ = pd.qcut(\n",
        "                    df[col], q=n_bins, labels=False, retbins=True, duplicates=\"drop\"\n",
        "                )\n",
        "                binned_col.name = f\"{col}_binned\"\n",
        "                # One-hot encode the bins\n",
        "                binned_one_hot = pd.get_dummies(\n",
        "                    binned_col, prefix=f\"{col}_bin\", drop_first=False, dummy_na=False\n",
        "                )\n",
        "                processed_parts.append(binned_one_hot)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not bin column '{col}'. Error: {e}\")\n",
        "                # Optionally keep the original column if binning fails, or handle differently\n",
        "                # processed_parts.append(df[[col]]) # Keep original if needed\n",
        "            finally:\n",
        "                # Drop original binned column regardless of success/failure if it exists\n",
        "                if col in df.columns:\n",
        "                    df = df.drop(columns=[col])\n",
        "\n",
        "    # 3. Ordinal Encode Features\n",
        "    for col, mapping in ordinal_features_map.items():\n",
        "        if col in df.columns:\n",
        "            ordinal_encoded = (\n",
        "                df[col].map(mapping).fillna(-1)\n",
        "            )  # Use -1 or another value for unknowns/NaNs\n",
        "            ordinal_encoded.name = f\"{col}_encoded\"\n",
        "            processed_parts.append(ordinal_encoded)\n",
        "            df = df.drop(columns=[col])\n",
        "\n",
        "    # 4. Keep Binary Features (Passthrough)\n",
        "    valid_binary = [col for col in binary_features if col in df.columns]\n",
        "    if valid_binary:\n",
        "        processed_parts.append(df[valid_binary])\n",
        "        df = df.drop(columns=valid_binary)  # Drop after adding to processed parts\n",
        "\n",
        "    # 5. Keep Original Numerical Features (will be scaled later)\n",
        "    # Ensure we only try to select columns that actually exist\n",
        "    valid_numerical_original = [\n",
        "        col for col in numerical_features_to_scale if col in df.columns\n",
        "    ]\n",
        "    numerical_original_df = df[\n",
        "        valid_numerical_original\n",
        "    ].copy()  # Keep these aside for now\n",
        "    if valid_numerical_original:\n",
        "        df = df.drop(columns=valid_numerical_original)  # Drop from main df\n",
        "\n",
        "    # --- Create Custom Features ---\n",
        "    # Select columns needed for custom features - ensure they exist\n",
        "    cols_for_custom = [\n",
        "        col\n",
        "        for col in binary_features\n",
        "        + numerical_features_to_scale\n",
        "        + list(ordinal_features_map.keys())\n",
        "        + continuous_for_binning\n",
        "        if col in df_raw.columns  # Check against original df\n",
        "    ]\n",
        "    custom_features_df = create_domain_features_pandas(df_raw[cols_for_custom])\n",
        "\n",
        "    # --- Combine all parts before scaling ---\n",
        "    df_combined = pd.concat(\n",
        "        processed_parts + [numerical_original_df, custom_features_df], axis=1\n",
        "    )\n",
        "\n",
        "    # 6. Standard Scale Numerical and Custom Features\n",
        "    # Identify all numerical columns to scale *after* custom features are added\n",
        "    cols_to_scale = list(numerical_original_df.columns) + list(\n",
        "        custom_features_df.columns\n",
        "    )\n",
        "    valid_cols_to_scale = [col for col in cols_to_scale if col in df_combined.columns]\n",
        "\n",
        "    if valid_cols_to_scale:\n",
        "        # Calculate mean and std deviation (add small epsilon to std dev for stability)\n",
        "        means = df_combined[valid_cols_to_scale].mean()\n",
        "        stds = (\n",
        "            df_combined[valid_cols_to_scale].std() + 1e-8\n",
        "        )  # Add epsilon to avoid division by zero\n",
        "\n",
        "        # Apply scaling\n",
        "        df_combined[valid_cols_to_scale] = (\n",
        "            df_combined[valid_cols_to_scale] - means\n",
        "        ) / stds\n",
        "\n",
        "    # Handle any remaining columns (e.g., ID column if not dropped earlier, or columns missed)\n",
        "    # In this version, we assume unhandled columns are dropped implicitly by not selecting them.\n",
        "    # If you need to keep other columns, adjust the logic.\n",
        "\n",
        "    return df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "908030e5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7492 - loss: 0.5109 - val_accuracy: 0.7486 - val_loss: 0.5110\n",
            "Epoch 2/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7492 - loss: 0.5109 - val_accuracy: 0.7486 - val_loss: 0.5110\n",
            "Epoch 2/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - accuracy: 0.7546 - loss: 0.4990 - val_accuracy: 0.7520 - val_loss: 0.5052\n",
            "Epoch 3/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 999us/step - accuracy: 0.7546 - loss: 0.4990 - val_accuracy: 0.7520 - val_loss: 0.5052\n",
            "Epoch 3/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - accuracy: 0.7562 - loss: 0.4965 - val_accuracy: 0.7513 - val_loss: 0.5050\n",
            "Epoch 4/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 984us/step - accuracy: 0.7562 - loss: 0.4965 - val_accuracy: 0.7513 - val_loss: 0.5050\n",
            "Epoch 4/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - accuracy: 0.7574 - loss: 0.4951 - val_accuracy: 0.7481 - val_loss: 0.5067\n",
            "Epoch 5/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 950us/step - accuracy: 0.7574 - loss: 0.4951 - val_accuracy: 0.7481 - val_loss: 0.5067\n",
            "Epoch 5/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - accuracy: 0.7589 - loss: 0.4927 - val_accuracy: 0.7500 - val_loss: 0.5060\n",
            "Epoch 6/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 952us/step - accuracy: 0.7589 - loss: 0.4927 - val_accuracy: 0.7500 - val_loss: 0.5060\n",
            "Epoch 6/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7600 - loss: 0.4919 - val_accuracy: 0.7521 - val_loss: 0.5047\n",
            "Epoch 7/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7600 - loss: 0.4919 - val_accuracy: 0.7521 - val_loss: 0.5047\n",
            "Epoch 7/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 915us/step - accuracy: 0.7596 - loss: 0.4911 - val_accuracy: 0.7504 - val_loss: 0.5052\n",
            "Epoch 8/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 915us/step - accuracy: 0.7596 - loss: 0.4911 - val_accuracy: 0.7504 - val_loss: 0.5052\n",
            "Epoch 8/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - accuracy: 0.7603 - loss: 0.4892 - val_accuracy: 0.7497 - val_loss: 0.5089\n",
            "Epoch 9/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - accuracy: 0.7603 - loss: 0.4892 - val_accuracy: 0.7497 - val_loss: 0.5089\n",
            "Epoch 9/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - accuracy: 0.7621 - loss: 0.4877 - val_accuracy: 0.7513 - val_loss: 0.5076\n",
            "Epoch 10/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - accuracy: 0.7621 - loss: 0.4877 - val_accuracy: 0.7513 - val_loss: 0.5076\n",
            "Epoch 10/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - accuracy: 0.7632 - loss: 0.4861 - val_accuracy: 0.7510 - val_loss: 0.5073\n",
            "Epoch 11/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step - accuracy: 0.7632 - loss: 0.4861 - val_accuracy: 0.7510 - val_loss: 0.5073\n",
            "Epoch 11/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909us/step - accuracy: 0.7648 - loss: 0.4849 - val_accuracy: 0.7522 - val_loss: 0.5078\n",
            "Epoch 12/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 909us/step - accuracy: 0.7648 - loss: 0.4849 - val_accuracy: 0.7522 - val_loss: 0.5078\n",
            "Epoch 12/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step - accuracy: 0.7651 - loss: 0.4837 - val_accuracy: 0.7502 - val_loss: 0.5096\n",
            "Epoch 13/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897us/step - accuracy: 0.7651 - loss: 0.4837 - val_accuracy: 0.7502 - val_loss: 0.5096\n",
            "Epoch 13/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893us/step - accuracy: 0.7650 - loss: 0.4810 - val_accuracy: 0.7483 - val_loss: 0.5110\n",
            "Epoch 14/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893us/step - accuracy: 0.7650 - loss: 0.4810 - val_accuracy: 0.7483 - val_loss: 0.5110\n",
            "Epoch 14/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step - accuracy: 0.7681 - loss: 0.4797 - val_accuracy: 0.7512 - val_loss: 0.5121\n",
            "Epoch 15/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step - accuracy: 0.7681 - loss: 0.4797 - val_accuracy: 0.7512 - val_loss: 0.5121\n",
            "Epoch 15/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - accuracy: 0.7677 - loss: 0.4780 - val_accuracy: 0.7478 - val_loss: 0.5117\n",
            "Epoch 16/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - accuracy: 0.7677 - loss: 0.4780 - val_accuracy: 0.7478 - val_loss: 0.5117\n",
            "Epoch 16/40\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - accuracy: 0.7679 - loss: 0.4754 - val_accuracy: 0.7490 - val_loss: 0.5123\n",
            "\u001b[1m663/663\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 953us/step - accuracy: 0.7679 - loss: 0.4754 - val_accuracy: 0.7490 - val_loss: 0.5123\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A080FE2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A080FE2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A080FE2840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - accuracy: 0.7521 - loss: 0.5047\n",
            "\u001b[1m332/332\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - accuracy: 0.7521 - loss: 0.5047\n",
            "Test accuracy: 0.752\n",
            "Test loss: 0.505\n",
            "Test accuracy: 0.752\n",
            "Test loss: 0.505\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "X_train_raw = pd.read_csv(\"data/X_train.csv\")\n",
        "y_train = pd.read_csv(\"data/y_train.csv\")\n",
        "X_train = X_train_raw.drop(\"ID\", axis=1)\n",
        "\n",
        "# Extract the correct target variable and encode it to numeric (No=0, Yes=1)\n",
        "y = y_train[\"Diabetes\"].map({\"No\": 0, \"Yes\": 1})\n",
        "\n",
        "X_train.drop(columns=[\"Gender\"], inplace=True)\n",
        "# Save feature names before converting to numpy so we can\n",
        "feature_names = X_train.columns.tolist()\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# X_train = preprocess_data_pandas(X_train)\n",
        "\n",
        "test_size = 0.2\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train, y, test_size=test_size, random_state=42\n",
        ")\n",
        "\n",
        "regularizer = None\n",
        "act = \"relu\"\n",
        "model = Sequential(\n",
        "    [\n",
        "        Input(shape=(X_train_split.shape[1],)),\n",
        "        # Dense(1024, activation=act, kernel_regularizer=regularizer),\n",
        "        # Dense(512, activation=act, kernel_regularizer=regularizer),\n",
        "        # Dense(256, activation=act, kernel_regularizer=regularizer),\n",
        "        Dense(128, activation=act, kernel_regularizer=regularizer),\n",
        "        Dense(64, activation=act, kernel_regularizer=regularizer),\n",
        "        Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "compile_loss = \"binary_crossentropy\"\n",
        "compile_metric = \"accuracy\"\n",
        "model.compile(\n",
        "    optimizer=Adam(),  # type: ignore\n",
        "    loss=compile_loss,\n",
        "    metrics=[compile_metric],\n",
        ")\n",
        "\n",
        "model_name = \"\"\n",
        "with mlflow.start_run() as run:\n",
        "    history = model.fit(\n",
        "        X_train_split,\n",
        "        y_train_split,\n",
        "        epochs=40,\n",
        "        batch_size=64,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "        ],\n",
        "        validation_data=(X_val_split, y_val_split),\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(X_val_split, y_val_split)\n",
        "    print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "    print(f\"Test loss: {test_loss:.3f}\")\n",
        "\n",
        "    mlflow.log_param(\"test_size\", test_size)\n",
        "    mlflow.log_param(\"compiler_loss\", compile_loss)\n",
        "    mlflow.log_param(\"compiler_metric\", compile_metric)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aml-competition-3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
